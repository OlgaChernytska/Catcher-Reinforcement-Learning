{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ple import PLE\n",
    "from ple.games.catcher import Catcher\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = 4\n",
    "dim_hidden = 4\n",
    "dim_out = 3\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1_value = np.random.randn(dim_in, dim_hidden) \n",
    "w2_value = np.random.randn(dim_hidden, dim_out) \n",
    "\n",
    "# Randomly initialize biases\n",
    "b1_value = np.random.randn(dim_hidden) \n",
    "b2_value = np.random.randn(dim_out) \n",
    "\n",
    "# Convert to pyTorch data sturture\n",
    "w1 = torch.from_numpy(w1_value).type(torch.FloatTensor).requires_grad_(True)\n",
    "w2 = torch.from_numpy(w2_value).type(torch.FloatTensor).requires_grad_(True)\n",
    "b1 = torch.from_numpy(b1_value).type(torch.FloatTensor).requires_grad_(True)\n",
    "b2 = torch.from_numpy(b2_value).type(torch.FloatTensor).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(logs):\n",
    "    learning_rate = 0.25\n",
    "    \n",
    "    action_set = p.getActionSet()\n",
    "    criterion = nn.MSELoss()\n",
    "    tanh = nn.Tanh()\n",
    "\n",
    "    # reshape data\n",
    "    x_value = np.array(logs[['player_x', 'player_vel', 'fruit_x', 'fruit_y']])\n",
    "    x = torch.from_numpy(x_value).type(torch.FloatTensor)\n",
    "        \n",
    "    # forward pass\n",
    "    x_hidden = x.mm(w1) + b1\n",
    "    x_hidden_act = tanh(x_hidden) \n",
    "    y_pred = x_hidden_act.mm(w2) + b2\n",
    "       \n",
    "    # compute y\n",
    "    actions = np.array(logs['action'])\n",
    "    rewards = np.array(logs['reward'])\n",
    "    y = torch.tensor(y_pred).detach()\n",
    "    for i in range(len(actions)):\n",
    "        y[i, action_set.index(actions[i])] = rewards[i]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = criterion(y_pred, y)\n",
    "        \n",
    "    # backprop\n",
    "    loss.backward()\n",
    "        \n",
    "    # update weights using gradient descent  \n",
    "    w1.data -= learning_rate * w1.grad\n",
    "    w2.data -= learning_rate * w2.grad\n",
    "    b1.data -= learning_rate * b1.grad\n",
    "    b2.data -= learning_rate * b2.grad\n",
    "\n",
    "    # manually zero the gradients\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    b2.grad.zero_()      \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize game\n",
    "width = 100\n",
    "height = 100\n",
    "game = Catcher(width, height, init_lives=1)\n",
    "p = PLE(game, fps=30, frame_skip=3, num_steps=1, force_fps=True, display_screen=False)\n",
    "p.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def pick_action(self, state, w1, b1, w2, b2, epsilon):\n",
    "        \n",
    "        rand = np.random.rand()\n",
    "        if rand<epsilon:\n",
    "            # exploration\n",
    "            action = self.actions[np.random.randint(0, len(self.actions))]\n",
    "        else: \n",
    "            # predict action q-values with NN\n",
    "            x = np.reshape(list(state.values()), (1,4))\n",
    "            x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "            x_hidden = x.mm(w1) + b1\n",
    "            tanh = nn.Tanh()\n",
    "            x_hidden_act = tanh(x_hidden) \n",
    "            y_pred = x_hidden_act.mm(w2) + b2\n",
    "            # choose action with highest q-value\n",
    "            action_index = np.argmax(np.reshape(y_pred.detach().numpy(), (3)))\n",
    "            action = self.actions[action_index]\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_batch(epsilon=0.5):\n",
    "    n_episodes = 20\n",
    "    n_timestamps = 200\n",
    "    agent = Agent(p.getActionSet())\n",
    "    \n",
    "    episode_rewards = []\n",
    "    logs = pd.DataFrame()\n",
    "    \n",
    "    for episode_i in range(n_episodes):\n",
    "        p.reset_game()\n",
    "        episode_reward = 0\n",
    "        state = game.getGameState()\n",
    "        \n",
    "        for timestamp in range(n_timestamps):\n",
    "            action = agent.pick_action(state, w1, b1, w2, b2, epsilon)\n",
    "            \n",
    "            reward = p.act(action)\n",
    "            \n",
    "            state['episode'] = episode_i\n",
    "            state['action'] = action\n",
    "            state['reward'] = reward\n",
    "            logs = pd.concat((logs, pd.DataFrame(state, index = [0])))\n",
    "            state = game.getGameState()\n",
    "            \n",
    "            if p.game_over():\n",
    "                episode_reward += -1\n",
    "                break\n",
    "    \n",
    "    return logs\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: mean reward: -5.700\n",
      "Epoch   2: mean reward: -5.850\n",
      "Epoch   3: mean reward: -5.700\n",
      "Epoch   4: mean reward: -5.550\n",
      "Epoch   5: mean reward: -5.600\n",
      "Epoch   6: mean reward: -5.550\n",
      "Epoch   7: mean reward: -5.650\n",
      "Epoch   8: mean reward: -5.700\n",
      "Epoch   9: mean reward: -5.550\n",
      "Epoch  10: mean reward: -5.550\n",
      "Epsilon changed to 0.4750\n",
      "Epoch  11: mean reward: -5.550\n",
      "Epoch  12: mean reward: -5.550\n",
      "Epoch  13: mean reward: -5.650\n",
      "Epoch  14: mean reward: -5.200\n",
      "Epoch  15: mean reward: -5.350\n",
      "Epoch  16: mean reward: -5.400\n",
      "Epoch  17: mean reward: -5.600\n",
      "Epoch  18: mean reward: -5.550\n",
      "Epoch  19: mean reward: -5.750\n",
      "Epoch  20: mean reward: -5.600\n",
      "Epsilon changed to 0.4512\n",
      "Epoch  21: mean reward: -5.600\n",
      "Epoch  22: mean reward: -5.850\n",
      "Epoch  23: mean reward: -5.500\n",
      "Epoch  24: mean reward: -5.400\n",
      "Epoch  25: mean reward: -5.650\n",
      "Epoch  26: mean reward: -5.700\n",
      "Epoch  27: mean reward: -5.700\n",
      "Epoch  28: mean reward: -5.500\n",
      "Epoch  29: mean reward: -5.600\n",
      "Epoch  30: mean reward: -5.800\n",
      "Epsilon changed to 0.4287\n",
      "Epoch  31: mean reward: -5.150\n",
      "Epoch  32: mean reward: -5.200\n",
      "Epoch  33: mean reward: -5.550\n",
      "Epoch  34: mean reward: -5.550\n",
      "Epoch  35: mean reward: -5.800\n",
      "Epoch  36: mean reward: -5.500\n",
      "Epoch  37: mean reward: -5.700\n",
      "Epoch  38: mean reward: -5.650\n",
      "Epoch  39: mean reward: -5.500\n",
      "Epoch  40: mean reward: -5.400\n",
      "Epsilon changed to 0.4073\n",
      "Epoch  41: mean reward: -5.700\n",
      "Epoch  42: mean reward: -5.500\n",
      "Epoch  43: mean reward: -5.400\n",
      "Epoch  44: mean reward: -5.150\n",
      "Epoch  45: mean reward: -5.900\n",
      "Epoch  46: mean reward: -5.500\n",
      "Epoch  47: mean reward: -5.550\n",
      "Epoch  48: mean reward: -5.650\n",
      "Epoch  49: mean reward: -5.650\n",
      "Epoch  50: mean reward: -5.500\n",
      "Epsilon changed to 0.3869\n",
      "Epoch  51: mean reward: -5.500\n",
      "Epoch  52: mean reward: -5.550\n",
      "Epoch  53: mean reward: -5.900\n",
      "Epoch  54: mean reward: -5.500\n",
      "Epoch  55: mean reward: -5.800\n",
      "Epoch  56: mean reward: -5.650\n",
      "Epoch  57: mean reward: -5.650\n",
      "Epoch  58: mean reward: -5.700\n",
      "Epoch  59: mean reward: -5.650\n",
      "Epoch  60: mean reward: -5.650\n",
      "Epsilon changed to 0.3675\n",
      "Epoch  61: mean reward: -5.400\n",
      "Epoch  62: mean reward: -5.550\n",
      "Epoch  63: mean reward: -5.350\n",
      "Epoch  64: mean reward: -5.600\n",
      "Epoch  65: mean reward: -5.300\n",
      "Epoch  66: mean reward: -5.650\n",
      "Epoch  67: mean reward: -5.950\n",
      "Epoch  68: mean reward: -5.550\n",
      "Epoch  69: mean reward: -5.400\n",
      "Epoch  70: mean reward: -5.150\n",
      "Epsilon changed to 0.3492\n",
      "Epoch  71: mean reward: -5.750\n",
      "Epoch  72: mean reward: -5.600\n",
      "Epoch  73: mean reward: -4.900\n",
      "Epoch  74: mean reward: -5.150\n",
      "Epoch  75: mean reward: -5.350\n",
      "Epoch  76: mean reward: -5.800\n",
      "Epoch  77: mean reward: -5.700\n",
      "Epoch  78: mean reward: -5.700\n",
      "Epoch  79: mean reward: -5.900\n",
      "Epoch  80: mean reward: -5.850\n",
      "Epsilon changed to 0.3317\n",
      "Epoch  81: mean reward: -5.950\n",
      "Epoch  82: mean reward: -5.750\n",
      "Epoch  83: mean reward: -5.700\n",
      "Epoch  84: mean reward: -5.450\n",
      "Epoch  85: mean reward: -5.450\n",
      "Epoch  86: mean reward: -5.350\n",
      "Epoch  87: mean reward: -5.500\n",
      "Epoch  88: mean reward: -5.600\n",
      "Epoch  89: mean reward: -5.500\n",
      "Epoch  90: mean reward: -5.500\n",
      "Epsilon changed to 0.3151\n",
      "Epoch  91: mean reward: -5.650\n",
      "Epoch  92: mean reward: -5.850\n",
      "Epoch  93: mean reward: -5.600\n",
      "Epoch  94: mean reward: -5.550\n",
      "Epoch  95: mean reward: -5.500\n",
      "Epoch  96: mean reward: -5.750\n",
      "Epoch  97: mean reward: -5.700\n",
      "Epoch  98: mean reward: -5.700\n",
      "Epoch  99: mean reward: -5.650\n",
      "Epoch 100: mean reward: -5.750\n",
      "Epsilon changed to 0.2994\n",
      "Epoch 101: mean reward: -5.750\n",
      "Epoch 102: mean reward: -5.600\n",
      "Epoch 103: mean reward: -5.650\n",
      "Epoch 104: mean reward: -5.450\n",
      "Epoch 105: mean reward: -5.600\n",
      "Epoch 106: mean reward: -5.600\n",
      "Epoch 107: mean reward: -5.850\n",
      "Epoch 108: mean reward: -5.800\n",
      "Epoch 109: mean reward: -5.450\n",
      "Epoch 110: mean reward: -5.600\n",
      "Epsilon changed to 0.2844\n",
      "Epoch 111: mean reward: -5.400\n",
      "Epoch 112: mean reward: -5.700\n",
      "Epoch 113: mean reward: -5.600\n",
      "Epoch 114: mean reward: -5.250\n",
      "Epoch 115: mean reward: -5.700\n",
      "Epoch 116: mean reward: -5.700\n",
      "Epoch 117: mean reward: -5.650\n",
      "Epoch 118: mean reward: -5.750\n",
      "Epoch 119: mean reward: -5.900\n",
      "Epoch 120: mean reward: -5.350\n",
      "Epsilon changed to 0.2702\n",
      "Epoch 121: mean reward: -5.900\n",
      "Epoch 122: mean reward: -5.400\n",
      "Epoch 123: mean reward: -5.500\n",
      "Epoch 124: mean reward: -5.600\n",
      "Epoch 125: mean reward: -5.800\n",
      "Epoch 126: mean reward: -5.600\n",
      "Epoch 127: mean reward: -5.500\n",
      "Epoch 128: mean reward: -5.550\n",
      "Epoch 129: mean reward: -5.500\n",
      "Epoch 130: mean reward: -5.700\n",
      "Epsilon changed to 0.2567\n",
      "Epoch 131: mean reward: -5.750\n",
      "Epoch 132: mean reward: -5.350\n",
      "Epoch 133: mean reward: -5.250\n",
      "Epoch 134: mean reward: -5.300\n",
      "Epoch 135: mean reward: -5.400\n",
      "Epoch 136: mean reward: -5.700\n",
      "Epoch 137: mean reward: -5.600\n",
      "Epoch 138: mean reward: -5.450\n",
      "Epoch 139: mean reward: -5.550\n",
      "Epoch 140: mean reward: -5.350\n",
      "Epsilon changed to 0.2438\n",
      "Epoch 141: mean reward: -5.800\n",
      "Epoch 142: mean reward: -5.650\n",
      "Epoch 143: mean reward: -5.650\n",
      "Epoch 144: mean reward: -5.450\n",
      "Epoch 145: mean reward: -5.800\n",
      "Epoch 146: mean reward: -5.550\n",
      "Epoch 147: mean reward: -5.700\n",
      "Epoch 148: mean reward: -5.400\n",
      "Epoch 149: mean reward: -5.900\n",
      "Epoch 150: mean reward: -5.400\n",
      "Epsilon changed to 0.2316\n",
      "Epoch 151: mean reward: -5.750\n",
      "Epoch 152: mean reward: -5.600\n",
      "Epoch 153: mean reward: -5.600\n",
      "Epoch 154: mean reward: -5.750\n",
      "Epoch 155: mean reward: -5.700\n",
      "Epoch 156: mean reward: -5.800\n",
      "Epoch 157: mean reward: -5.600\n",
      "Epoch 158: mean reward: -5.700\n",
      "Epoch 159: mean reward: -5.550\n",
      "Epoch 160: mean reward: -5.600\n",
      "Epsilon changed to 0.2201\n",
      "Epoch 161: mean reward: -5.700\n",
      "Epoch 162: mean reward: -5.350\n",
      "Epoch 163: mean reward: -5.650\n",
      "Epoch 164: mean reward: -5.450\n",
      "Epoch 165: mean reward: -5.450\n",
      "Epoch 166: mean reward: -5.550\n",
      "Epoch 167: mean reward: -5.400\n",
      "Epoch 168: mean reward: -5.250\n",
      "Epoch 169: mean reward: -5.700\n",
      "Epoch 170: mean reward: -5.600\n",
      "Epsilon changed to 0.2091\n",
      "Epoch 171: mean reward: -5.750\n",
      "Epoch 172: mean reward: -5.950\n",
      "Epoch 173: mean reward: -5.500\n",
      "Epoch 174: mean reward: -5.300\n",
      "Epoch 175: mean reward: -5.350\n",
      "Epoch 176: mean reward: -5.650\n",
      "Epoch 177: mean reward: -5.650\n",
      "Epoch 178: mean reward: -5.600\n",
      "Epoch 179: mean reward: -5.450\n",
      "Epoch 180: mean reward: -5.300\n",
      "Epsilon changed to 0.1986\n",
      "Epoch 181: mean reward: -5.400\n",
      "Epoch 182: mean reward: -5.900\n",
      "Epoch 183: mean reward: -5.750\n",
      "Epoch 184: mean reward: -5.400\n",
      "Epoch 185: mean reward: -5.850\n",
      "Epoch 186: mean reward: -5.950\n",
      "Epoch 187: mean reward: -5.400\n",
      "Epoch 188: mean reward: -5.150\n",
      "Epoch 189: mean reward: -5.800\n",
      "Epoch 190: mean reward: -5.850\n",
      "Epsilon changed to 0.1887\n",
      "Epoch 191: mean reward: -5.450\n",
      "Epoch 192: mean reward: -5.450\n",
      "Epoch 193: mean reward: -5.850\n",
      "Epoch 194: mean reward: -5.950\n",
      "Epoch 195: mean reward: -5.500\n",
      "Epoch 196: mean reward: -5.550\n",
      "Epoch 197: mean reward: -5.300\n",
      "Epoch 198: mean reward: -5.450\n",
      "Epoch 199: mean reward: -5.650\n",
      "Epoch 200: mean reward: -5.350\n",
      "Epsilon changed to 0.1792\n",
      "Epoch 201: mean reward: -5.450\n",
      "Epoch 202: mean reward: -5.900\n",
      "Epoch 203: mean reward: -5.900\n",
      "Epoch 204: mean reward: -5.600\n",
      "Epoch 205: mean reward: -5.400\n",
      "Epoch 206: mean reward: -5.850\n",
      "Epoch 207: mean reward: -5.450\n",
      "Epoch 208: mean reward: -5.300\n",
      "Epoch 209: mean reward: -5.800\n",
      "Epoch 210: mean reward: -5.350\n",
      "Epsilon changed to 0.1703\n",
      "Epoch 211: mean reward: -5.300\n",
      "Epoch 212: mean reward: -5.600\n",
      "Epoch 213: mean reward: -5.800\n",
      "Epoch 214: mean reward: -5.800\n",
      "Epoch 215: mean reward: -5.600\n",
      "Epoch 216: mean reward: -5.800\n",
      "Epoch 217: mean reward: -5.750\n",
      "Epoch 218: mean reward: -5.600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-38f30377479b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %3d: mean reward: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a982dad3e7c7>\u001b[0m in \u001b[0;36mplay_batch\u001b[0;34m(epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGameState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                        copy=copy)\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5202\u001b[0m             b = make_block(\n\u001b[0;32m-> 5203\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5204\u001b[0m                 placement=placement)\n\u001b[1;32m   5205\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5330\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   5331\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 5332\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   5333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5330\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   5331\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 5332\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   5333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m   5630\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5631\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[0;32m-> 5632\u001b[0;31m                                        fill_value=fill_value)\n\u001b[0m\u001b[1;32m   5633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5634\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetimetz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_interval_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_interval_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIntervalDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/dtypes.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntervalDtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "for i in range(500):\n",
    "    logs = play_batch()\n",
    "    print('Epoch %3d: mean reward: %.3f' % (i+1, logs['reward'].sum()/logs['episode'].nunique()))\n",
    "    update_weights(logs)\n",
    "    if (i+1)%10 == 0:\n",
    "        epsilon *= 0.95\n",
    "        print('Epsilon changed to %.4f' % epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
